# Day 3: Neural Networks & Deep Learning - Cheatsheet

## üß† Quick Reference Guide for Neural Networks & Deep Learning

---

## 1. üìê Neural Network Architectures

### Architecture Types & Use Cases

| **Architecture** | **Best For** | **Key Features** | **Example** |
|------------------|--------------|------------------|-------------|
| **MLP/FNN** | Tabular data | Fully connected, feedforward | Customer churn, fraud detection |
| **CNN** | Images, spatial data | Convolution, pooling, parameter sharing | Image classification, object detection |
| **RNN/LSTM** | Sequential data | Memory, temporal dependencies | NLP, speech, time series |
| **Transformer** | Sequential data | Attention mechanism, parallel processing | GPT, BERT, machine translation |

### Layer Components

```
Input Layer ‚Üí Hidden Layers ‚Üí Output Layer

Hidden Layer Computation:
output = activation(Œ£(weight √ó input) + bias)

Output Layer Size:
‚Ä¢ Binary classification: 1 node (sigmoid)
‚Ä¢ Multi-class: N nodes (softmax)  
‚Ä¢ Regression: 1 node (linear)
```

### Network Capacity
- **Width**: Neurons per layer (‚Üë capacity)
- **Depth**: Number of layers (‚Üë hierarchical features)
- **Rule**: More parameters = more capacity BUT higher overfitting risk

---

## 2. ‚ö° Activation Functions

### Quick Selection Guide

| **Use Case** | **Activation** | **Formula** | **Range** |
|--------------|----------------|-------------|-----------|
| **Hidden layers (default)** | ReLU | `max(0, x)` | [0, ‚àû) |
| **Hidden layers (dying ReLU)** | Leaky ReLU | `max(0.01x, x)` | (-‚àû, ‚àû) |
| **Binary output** | Sigmoid | `1/(1+e^(-x))` | (0, 1) |
| **Multi-class output** | Softmax | `e^(xi)/Œ£e^(xj)` | [0, 1], Œ£=1 |
| **Regression output** | Linear | `x` | (-‚àû, ‚àû) |
| **Modern transformers** | GELU | `x √ó Œ¶(x)` | (-‚àû, ‚àû) |
| **RNN hidden** | Tanh | `(e^x-e^(-x))/(e^x+e^(-x))` | (-1, 1) |

### Activation Function Properties

| **Function** | **Pros** | **Cons** | **When to Use** |
|--------------|----------|----------|-----------------|
| **ReLU** | Fast, sparse, no vanishing gradient | Dead neurons | Default choice |
| **Leaky ReLU** | Prevents dead neurons | Slight negative slope | When ReLU neurons dying |
| **Sigmoid** | Smooth, probabilistic | Vanishing gradient, not zero-centered | Binary classification output |
| **Tanh** | Zero-centered | Vanishing gradient | RNN hidden layers |
| **Softmax** | Probability distribution | Only for output | Multi-class output |

---

## 3. üìâ Loss Functions

### Classification Losses

| **Task** | **Loss Function** | **Formula** | **Output Activation** |
|----------|-------------------|-------------|----------------------|
| **Binary** | Binary Cross-Entropy | `-[y log(≈∑) + (1-y)log(1-≈∑)]` | Sigmoid |
| **Multi-class** | Categorical Cross-Entropy | `-Œ£(yi √ó log(≈∑i))` | Softmax |
| **Multi-class (int labels)** | Sparse Categorical | Same as above | Softmax |
| **Imbalanced** | Focal Loss | `-Œ±(1-≈∑)^Œ≥ log(≈∑)` | Sigmoid/Softmax |

### Regression Losses

| **Scenario** | **Loss Function** | **Formula** | **When to Use** |
|--------------|-------------------|-------------|-----------------|
| **Standard** | MSE | `(1/n)Œ£(y-≈∑)¬≤` | Normal distribution, penalize large errors |
| **Outliers present** | MAE | `(1/n)Œ£|y-≈∑|` | Robust to outliers |
| **Balanced** | Huber | MSE + MAE hybrid | Balance smoothness & robustness |

---

## 4. üéØ Optimizers

### Optimizer Selection Guide

| **Optimizer** | **Learning Rate** | **Use Case** | **Pros** | **Cons** |
|---------------|-------------------|--------------|----------|----------|
| **Adam** | 0.001 | Default choice, transformers | Fast convergence, adaptive LR | May generalize worse |
| **SGD + Momentum** | 0.01-0.1 | Best generalization, CV | Best final performance | Slow convergence, needs tuning |
| **AdamW** | 0.001 | Modern transformers | Better regularization than Adam | More complex |
| **RMSprop** | 0.001 | RNNs | Good for RNNs | Less popular |

### Optimizer Formulas

```python
# SGD with Momentum
v = Œ≤ √ó v + gradient
w = w - lr √ó v

# Adam
m = Œ≤1 √ó m + (1-Œ≤1) √ó gradient      # First moment
v = Œ≤2 √ó v + (1-Œ≤2) √ó gradient¬≤     # Second moment  
w = w - lr √ó m/‚àö(v + Œµ)             # Weight update
```

### Learning Rate Scheduling

| **Method** | **When to Use** | **Implementation** |
|------------|-----------------|-------------------|
| **Step Decay** | Simple baseline | Reduce by 0.1 every N epochs |
| **Cosine Annealing** | Smooth decrease | Follow cosine curve |
| **ReduceLROnPlateau** | Adaptive | Reduce when validation stops improving |
| **Warmup** | Large batch, transformers | Gradually increase initially |

---

## 5. üîÑ Gradient Descent Variants

### Batch Size Selection

| **Batch Size** | **Pros** | **Cons** | **Use Case** |
|----------------|----------|----------|--------------|
| **1 (SGD)** | Fast, escapes local minima | Noisy, erratic | Large datasets, online learning |
| **16-128 (Mini-batch)** | Balanced speed/stability | Need tuning | **Standard practice** |
| **Full dataset (Batch)** | Smooth convergence | Slow, memory intensive | Small datasets only |

### Batch Size Guidelines
- **Start with**: 32 (good default)
- **Increase if**: Training is noisy, have more memory
- **Decrease if**: Overfitting, memory issues
- **Modern trend**: Large batches (512-4096) with LR scaling

---

## 6. üõ°Ô∏è Regularization Techniques

### Overfitting Detection
- **Signs**: Training ‚Üë, Validation ‚Üì
- **Training loss** ‚Üì, **Validation loss** ‚Üë
- Large gap between train/val performance

### Regularization Methods (Priority Order)

| **Priority** | **Method** | **Implementation** | **Typical Values** |
|--------------|------------|-------------------|-------------------|
| **1. Always** | Early Stopping | Monitor val_loss, patience=10-20 | Stop when no improvement |
| **2. Data** | Data Augmentation | Rotation, flip, crop (images) | Essential for CV |
| **3. Architecture** | Dropout | Randomly zero neurons | 0.5 (dense), 0.2 (CNN) |
| **4. Weights** | L2 Regularization | Add ŒªŒ£w¬≤ to loss | Œª = 0.001-0.01 |
| **5. Last resort** | Reduce Model Size | Fewer layers/neurons | If above doesn't work |

### Dropout Implementation

```python
# Dense layers
x = Dense(256)(x)
x = Dropout(0.5)(x)  # Drop 50% of neurons
x = Activation('relu')(x)

# CNN layers  
x = Conv2D(64, 3)(x)
x = Dropout(0.2)(x)  # Drop 20% of neurons
x = Activation('relu')(x)
```

### L1 vs L2 Regularization

| **Type** | **Formula** | **Effect** | **Use Case** |
|----------|-------------|------------|--------------|
| **L1** | `ŒªŒ£|w|` | Sparse weights (exactly 0) | Feature selection |
| **L2** | `ŒªŒ£w¬≤` | Small weights | General regularization |

---

## 7. üìä Normalization

### When to Use Each

| **Normalization** | **Best For** | **Normalizes** | **Batch Size** |
|-------------------|--------------|----------------|----------------|
| **Batch Norm** | CNNs, large batches | Across batch | ‚â•16 required |
| **Layer Norm** | Transformers, RNNs | Across features | Any size |
| **Group Norm** | Small batches, CV | Feature groups | Any size |

### BatchNorm vs LayerNorm

```python
# Batch Normalization (across batch dimension)
x_norm = (x - mean_batch) / sqrt(var_batch + Œµ)

# Layer Normalization (across feature dimension)  
x_norm = (x - mean_features) / sqrt(var_features + Œµ)
```

### Placement in Network

```python
# Common pattern
x = Dense(128)(x)
x = BatchNormalization()(x)  # After dense, before activation
x = Activation('relu')(x)
x = Dropout(0.5)(x)
```

---

## 8. üîÑ Transfer Learning

### Strategy Selection

| **Dataset Size** | **Domain Similarity** | **Strategy** | **Layers to Unfreeze** |
|------------------|----------------------|--------------|----------------------|
| **Small (<1K)** | High | Feature Extraction | 0% (freeze all) |
| **Medium (1K-10K)** | Medium | Fine-tuning | 25-50% (top layers) |
| **Large (>10K)** | Low | Full Fine-tuning | 100% (all layers) |

### Transfer Learning Workflow

```python
# 1. Feature Extraction (Small Data)
base_model.trainable = False
model.fit(X, y, epochs=10, lr=0.001)

# 2. Fine-tuning (Medium/Large Data)  
base_model.trainable = True
# Freeze early layers
for layer in base_model.layers[:100]:
    layer.trainable = False
    
model.compile(optimizer=Adam(lr=1e-5))  # Small LR!
model.fit(X, y, epochs=20)
```

### Learning Rate Strategy

| **Layer Type** | **Learning Rate** | **Reasoning** |
|----------------|-------------------|---------------|
| **Frozen layers** | 0 | Don't update |
| **Early layers** | 1e-5 | General features, small changes |
| **Middle layers** | 1e-4 | Domain adaptation |
| **New head** | 1e-3 | Random initialization, needs more updates |

### Popular Pre-trained Models

| **Domain** | **Model** | **Use Case** | **Size** |
|------------|-----------|--------------|----------|
| **Vision** | ResNet50 | General baseline | Medium |
| **Vision** | EfficientNet | Best accuracy/efficiency | Small-Large |
| **Vision** | MobileNet | Mobile/edge deployment | Small |
| **NLP** | BERT | Text understanding | Large |
| **NLP** | GPT | Text generation | Large |
| **NLP** | DistilBERT | Faster BERT | Medium |

---

## 9. ‚öñÔ∏è Deep Learning vs Classical ML

### Decision Framework

```
1. Data Type?
   ‚îú‚îÄ Tabular ‚Üí Classical ML (XGBoost/LightGBM)
   ‚îî‚îÄ Images/Text/Audio ‚Üí Deep Learning

2. Dataset Size?
   ‚îú‚îÄ <100K rows ‚Üí Classical ML  
   ‚îî‚îÄ >100K rows ‚Üí Consider both

3. Interpretability needed?
   ‚îú‚îÄ Yes ‚Üí Classical ML
   ‚îî‚îÄ No ‚Üí Deep Learning OK

4. Infrastructure/Time?
   ‚îú‚îÄ Limited ‚Üí Classical ML
   ‚îî‚îÄ Ample ‚Üí Deep Learning OK

5. Performance requirements?
   ‚îú‚îÄ Good enough ‚Üí Classical ML (faster)
   ‚îî‚îÄ State-of-art ‚Üí Deep Learning
```

### Use Classical ML When:
- ‚úÖ Tabular data (<100K rows)
- ‚úÖ Need interpretability  
- ‚úÖ Limited compute/time
- ‚úÖ Fast prototyping needed
- ‚úÖ Well-engineered features exist

### Use Deep Learning When:
- ‚úÖ Unstructured data (images/text/audio)
- ‚úÖ Large datasets (>100K samples)
- ‚úÖ Complex patterns expected
- ‚úÖ Transfer learning available
- ‚úÖ State-of-art performance needed

---

## 10. üõ†Ô∏è Implementation Checklist

### Model Building Checklist

```python
# 1. Data Preparation
‚ñ° Normalize/scale features
‚ñ° Handle missing values  
‚ñ° Train/val/test split
‚ñ° Data augmentation (if applicable)

# 2. Architecture Design
‚ñ° Choose appropriate architecture (MLP/CNN/RNN/Transformer)
‚ñ° Select activation functions (ReLU hidden, sigmoid/softmax output)
‚ñ° Add normalization (BatchNorm/LayerNorm)
‚ñ° Add dropout for regularization

# 3. Training Setup
‚ñ° Choose loss function (binary/categorical cross-entropy, MSE)
‚ñ° Select optimizer (Adam default, SGD for best performance)
‚ñ° Set learning rate (0.001 Adam, 0.01-0.1 SGD)
‚ñ° Configure callbacks (early stopping, LR scheduling)

# 4. Training Process
‚ñ° Monitor train/val curves
‚ñ° Check for overfitting (train‚Üë val‚Üì)
‚ñ° Apply regularization if needed
‚ñ° Save best model (not final)

# 5. Evaluation
‚ñ° Test on held-out data
‚ñ° Check for data leakage
‚ñ° Analyze failure cases
‚ñ° Compare with baseline
```

### Common Training Issues & Fixes

| **Problem** | **Symptoms** | **Solutions** |
|-------------|--------------|---------------|
| **Overfitting** | Train‚Üë Val‚Üì | Dropout, early stopping, more data |
| **Underfitting** | Both train/val low | More capacity, less regularization |
| **Vanishing gradients** | Loss plateaus early | ReLU, skip connections, BatchNorm |
| **Exploding gradients** | Loss spikes/NaN | Gradient clipping, lower LR |
| **Dead ReLUs** | Many 0 activations | Leaky ReLU, lower LR, better init |
| **Slow convergence** | Loss decreases slowly | Higher LR, Adam optimizer, BatchNorm |

---

## 11. üìù Interview Quick Answers

### Key Concepts (30 seconds each)

**Q: Why activation functions?**
A: "Without activation functions, neural network = linear regression. Activations introduce non-linearity, enabling universal approximation and deep learning."

**Q: ReLU vs Sigmoid?**  
A: "ReLU: Fast, no vanishing gradients, sparse. Use for hidden layers. Sigmoid: Smooth, outputs probabilities. Use for binary classification output."

**Q: When Adam vs SGD?**
A: "Adam: Fast prototyping, good default (lr=0.001). SGD: Best generalization, needs tuning (lr=0.01-0.1). I start with Adam, switch to SGD if time permits."

**Q: How prevent overfitting?**
A: "Priority order: (1) More data + augmentation, (2) Early stopping, (3) Dropout (0.5 dense, 0.2 CNN), (4) L2 regularization (0.01), (5) Reduce model size."

**Q: Transfer learning strategy?**
A: "Depends on data size and similarity. Small data: freeze base, train head. Medium data: unfreeze top layers with small LR. Large data: fine-tune everything."

**Q: Deep Learning vs Classical ML?**
A: "Tabular data <100K: Classical ML (XGBoost). Images/text or >100K samples: Deep Learning. Classical ML easier/faster, DL higher ceiling with sufficient data."

---

## 12. üî¢ Key Formulas

### Neural Network Forward Pass
```
z = W¬∑x + b           # Linear transformation
a = activation(z)     # Non-linear activation  
loss = L(y, ≈∑)       # Loss computation
```

### Backpropagation (Chain Rule)
```
‚àÇL/‚àÇW = ‚àÇL/‚àÇa √ó ‚àÇa/‚àÇz √ó ‚àÇz/‚àÇW
‚àÇL/‚àÇb = ‚àÇL/‚àÇa √ó ‚àÇa/‚àÇz √ó ‚àÇz/‚àÇb
```

### Common Activations
```
ReLU: f(x) = max(0, x)
Sigmoid: f(x) = 1/(1 + e^(-x))  
Tanh: f(x) = (e^x - e^(-x))/(e^x + e^(-x))
Softmax: f(xi) = e^(xi) / Œ£e^(xj)
```

### Optimization Updates
```
SGD: w = w - lr √ó ‚àÇL/‚àÇw
Momentum: v = Œ≤v + ‚àÇL/‚àÇw; w = w - lr √ó v
Adam: Combines momentum + adaptive learning rates
```

---

## 13. üéØ Performance Benchmarks

### Typical Training Times (Single GPU)
- **MNIST (60K images)**: 5-10 minutes
- **CIFAR-10 (50K images)**: 30-60 minutes  
- **ImageNet (1.3M images)**: 1-3 days
- **BERT (large text corpus)**: 1-4 days

### Memory Requirements
- **Parameters**: 4 bytes per parameter (float32)
- **Activations**: Batch size √ó layer size √ó 4 bytes
- **Gradients**: Same as parameters
- **Rule of thumb**: 2-3x parameter memory for training

### Accuracy Benchmarks
- **MNIST**: 99%+ (simple CNN)
- **CIFAR-10**: 95%+ (ResNet)
- **ImageNet**: 80%+ (EfficientNet)
- **BERT**: 90%+ (GLUE benchmark)

---

## 14. üîó Quick Reference Links

### Activation Functions
- Use **ReLU** by default for hidden layers
- Use **Leaky ReLU** if ReLU neurons die  
- Use **Sigmoid** for binary output
- Use **Softmax** for multi-class output
- Use **Linear** for regression output

### Optimizers
- Start with **Adam** (lr=0.001)
- Use **SGD + momentum** for best final performance
- Use **AdamW** for transformers
- Use **RMSprop** for RNNs

### Regularization
- Always use **early stopping**
- Use **dropout** (0.5 dense, 0.2 conv)
- Use **L2** regularization (0.001-0.01)
- Use **data augmentation** for images

### Architecture
- **MLP**: Tabular data
- **CNN**: Images, spatial data
- **RNN/LSTM**: Sequences, time series
- **Transformer**: Modern NLP, some vision

---

## üí° Pro Tips

1. **Always start simple**: Single layer ‚Üí Add complexity gradually
2. **Monitor train/val curves**: Most important debugging tool
3. **Use transfer learning**: Don't train from scratch if pre-trained available
4. **Save best model**: Use validation loss, not training loss
5. **Reproducibility**: Set random seeds for debugging
6. **Baseline first**: Compare against simple models (logistic regression)
7. **Feature engineering**: Still matters even in deep learning
8. **GPU utilization**: Batch size affects GPU efficiency
9. **Learning rate**: Most important hyperparameter to tune
10. **Patience**: Deep learning takes time; don't expect immediate results

---

**üéØ Remember**: Neural networks are function approximators. With enough data and compute, they can learn complex patterns. The art is in the architecture design and training process!
